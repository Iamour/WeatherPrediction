2020-6-30，9：00-12：00
昨天做到了任务5，今天先进行spark和hadoop环境配置，失败，原因未知
先往下做了任务7，安装依赖包，成功
   --队友提示，使用-i以后速度快了很多
下载了虚拟机vmware
突然发现spark和hadoop版本没对上，于是重新下载了Hadoop3.2和spark3.0
重新配置，失败
发现java环境配置出错，大二上学过Java所以太信任了以为没事，可能在调其他环境配置的时候破坏了Java环境配置
重新配置Java，成功
再去试spark，cmd上还是无法显示版本信息
推断可能是缺少Windows依赖包？搜索hadoop3.2依赖包无果
询问队友，阶段相似，同样卡住。已经完成的是下载了别的版本，如果下午还是不行就换版本下载
看到群里说可能是因为python版本太低，我下载的是3.8，下个更高版本试试

