2020-6-30，9：00-12：00
昨天做到了任务5，今天先进行spark和hadoop环境配置，失败，原因未知
先往下做了任务7，安装依赖包，成功
   --队友提示，使用-i以后速度快了很多
下载了虚拟机vmware
突然发现spark和hadoop版本没对上，于是重新下载了Hadoop3.2和spark3.0
重新配置，失败
发现java环境配置出错，大二上学过Java所以太信任了以为没事，可能在调其他环境配置的时候破坏了Java环境配置
重新配置Java，成功
再去试spark，cmd上还是无法显示版本信息
推断可能是缺少Windows依赖包？搜索hadoop3.2依赖包无果
询问队友，阶段相似，同样卡住。已经完成的是下载了别的版本，如果下午还是不行就换版本下载
看到群里说可能是因为python版本太低，我下载的是3.8，下个更高版本试试

2020-6-30 14：00-18：00
王迪 18301078
正在安装虚拟机的时候组里提出计划更动，要大家先跳过虚拟机往后进行
按照计划我负责微服务器，所以接下来是我关于微服务器的一些log
1. 成功配置Flask运行环境
2. 阅读网上关于Flask的教程，读了Flask的概述，环境，应用，路由，变量规则。有了初步了解
3. 成功启动Flask程序

反思：一开始操之过急，对很多基本概念没有很了解就往下进行，导致走了很多弯路。比如缺失关于python的基本理解
打算：踏实下来，结合基本的概念原理和实际操作一起学习。