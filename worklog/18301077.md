# 日志

by 18301077

## 2020.6.30

### 上午

1. Ubuntu上jdk1.8的安装：使用的是apt install openjdk-8-jdk，然后了解了jdk和openjdk的区别，重新在官网下载了Oracle jdk并安装。在一次重启中，wmware虚拟机黑屏，根据之前的解决方案，netsh winsock reset、移除虚拟机并重新添加、重启后问题解决。
2. spark集群：粗略的看了一下要求发现需要两个子节点，暂时跳过
3. flask安装：使用
   
       pip install flask -i https://pypi.tuna.tsinghua.edu.cn/simple 
    
    安装完成，并成功运行了 https://dormousehole.readthedocs.io/en/latest/ 中文文档入门教程中的helloworld 程序
4. 官网下载 tomcat 9.0.36 zip 包，并根据自带 RUNING.txt 的说明设置 CATALINA_HOME 环境变量，最后运行 startup.bat 成功启动tomcat，运行 shutdown.bat 成功关闭tomcat。

### 下午

配置Ubuntu下的Spark/Hadoop：

1. 起初直接使用vmware自带的共享文件夹来传输压缩包，发现无法使用，下载WinSCP连接Ubuntu虚拟机，最后连接成功。
2. 使用windows系统编辑好bash脚本，传输到Ubuntu中，执行中出现file not found，发现是Windows的行尾\r\n问题，使用vim转换后成功。
3. 运行spark时报错，经查是输入错误。错误改正后，发现访问拒绝，推测是远程登录有问题，暂未找到原因。

## 2020.7.1

### 上午

1. 从ubuntu文档、菜鸟教程等地方学习了基本的linux指令，如export等的作用，了解了环境变量的写法。再根据spark/hadoop官网以及文档重新进行配置。遇到了Call to localhost/127.0.0.1:9000 failed on connection exception: java.net.ConnectExcept报错，经查是NameNode进程没有启动，启动后正常，又遇到fs-put无法加载主类报错，正在查找原因。

### 下午

1. 查阅了hadoop fs指令的用法，发现上午的fs-put为指令错误，应为fs -put。然后运行hadoop测试程序时系统卡死，重启后打开资源监视器发现为内存不足。调整VMware的cpu核数和内存大小后运行成功。
2. 运行pyspark时提示找不到python，发现Ubuntu下默认为python3，pyspark调用的是python，最后使用

       sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150

   将默认的python3命令改为python，运行成功并筛选了初步的csv数据。
3. 参照spark官网的quick start，了解了如何使用pyspark进行数据处理
4. 初步了解时间序列分析