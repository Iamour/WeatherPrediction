# 日志

by 18301077-童路勤

## 2020.6.30

### 上午

- Ubuntu上jdk1.8的安装：使用的是apt install openjdk-8-jdk，然后了解了jdk和openjdk的区别，重新在官网下载了Oracle jdk并安装。在一次重启中，wmware虚拟机黑屏，根据之前的解决方案，netsh winsock reset、移除虚拟机并重新添加、重启后问题解决。
- spark集群：粗略的看了一下要求发现需要两个子节点，暂时跳过
- flask安装：使用
   
       pip install flask -i https://pypi.tuna.tsinghua.edu.cn/simple 
    
    安装完成，并成功运行了 https://dormousehole.readthedocs.io/en/latest/ 中文文档入门教程中的helloworld 程序
- 官网下载 tomcat 9.0.36 zip 包，并根据自带 RUNING.txt 的说明设置 CATALINA_HOME 环境变量，最后运行 startup.bat 成功启动tomcat，运行 shutdown.bat 成功关闭tomcat。

### 下午

配置Ubuntu下的Spark/Hadoop：

- 起初直接使用vmware自带的共享文件夹来传输压缩包，发现无法使用，下载WinSCP连接Ubuntu虚拟机，最后连接成功。
- 使用windows系统编辑好bash脚本，传输到Ubuntu中，执行中出现file not found，发现是Windows的行尾\r\n问题，使用vim转换后成功。
- 运行spark时报错，经查是输入错误。错误改正后，发现访问拒绝，推测是远程登录有问题，暂未找到原因。

## 2020.7.1

### 上午

- 从ubuntu文档、菜鸟教程等地方学习了基本的linux指令，如export等的作用，了解了环境变量的写法。再根据spark/hadoop官网以及文档重新进行配置。遇到了Call to localhost/127.0.0.1:9000 failed on connection exception: java.net.ConnectExcept报错，经查是NameNode进程没有启动，启动后正常，又遇到fs-put无法加载主类报错，正在查找原因。

### 下午

- 查阅了hadoop fs指令的用法，发现上午的fs-put为指令错误，应为fs -put。然后运行hadoop测试程序时系统卡死，重启后打开资源监视器发现为内存不足。调整VMware的cpu核数和内存大小后运行成功。
- 运行pyspark时提示找不到python，发现Ubuntu下默认为python3，pyspark调用的是python，最后使用

       sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150

   将默认的python3命令改为python，运行成功并筛选了初步的csv数据。
- 参照spark官网的quick start，了解了如何使用pyspark进行数据处理
- 初步了解时间序列分析

## 2020.7.2

### 上午

- 学习使用了pandas、statsmodels等库的API，尝试构建ARIMA模型，但效果不理想。需要继续理解模型的含义。

### 下午

- 使用初步构建的ARIMA模型对数据进行了预测。
- 暂时使用pmdarima库构建了简单的ARIMA模型，计划将接口设计好后使用statsmodels等库进行更详细的模型设计。

## 2020.7.3

### 上午

- 实现模型的接口类
- 继续研究ARIMA模型

### 下午

- 完成了ARIMA模型的demo，手动ARIMA和自动ARIMA，加载模型，传入日期，返回预测值并保存为csv文件。
- ARIMA模型效果十分一般，只能有大体趋势，mse在20-60左右，参数的取值还不太明确，需要进一步学习。

## 2020.7.3

- 尝试改进模型，没有结果

## 2020.7.4

- 继续改进模型。使用的是1980-01-01到2010-01-01的全部温度数据，建立ARIMA模型，发现预测MSE过高，预测值之间没有明显波动，呈直线，尝试使用Auto-ARIMA自动调参仍然无法解决，尝试使用季节性分解，仍无效果。想到使用历年01-01的数据，但感觉仅30多个数据无法得到有效信息，网络上已有的ARIMA预测也没有按年划分数据的例子，也无法解释“用历年02-29的数据预测今年02-29，”，因此很困惑。
- 优化了调用接口函数